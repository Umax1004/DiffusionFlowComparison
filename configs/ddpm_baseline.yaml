globals:
  seed: 42
  project_name: ddpm-celebahq
  run_name: unet3m-128-adamw8bit-ema

data:
  dataset: CelebA-HQ
  image_size: 128
  train:
    batch_size: 128
    split: train
    num_workers: 6
    persistent_workers: true
    pin_memory: true
  val:
    batch_size: 128
    split: validation
    num_workers: 4

model:
  _target_: diffusers.UNet2DModel
  sample_size: 128
  in_channels: 3
  out_channels: 3
  layers_per_block: 1
  block_out_channels: [32, 64, 128]
  down_block_types: 
    - AttnDownBlock2D
    - AttnDownBlock2D
    - AttnDownBlock2D
  up_block_types: 
    - AttnUpBlock2D
    - AttnUpBlock2D
    - AttnUpBlock2D

training:
  epochs: 1000
  lr: 1e-4
  betas: [0.9, 0.99]
  weight_decay: 0.01
  optimizer: 
    _target_: bitsandbytes.optim.AdamW8bit
  scheduler:
    warmup_ratio: 0.05
    min_lr_ratio: 0.1
  ema_decay: 0.9999
  grad_clip: 30.0
  loss_fn: mse

diffusion:
  _target_: diffusers.DDPMScheduler
  num_train_timesteps: 1000
  beta_schedule: linear

precision:
  use_scaler: true
  compile:
    mode: max-autotune
    fullgraph: false

eval:
  sample_every: 10000
  fid_every: 50000
  num_sample_images: 8
  fid_num_samples: null
  fid_batch_size: 128
  sample_steps: 250

logging:
  wandb:
    project: ${globals.project_name}
    name: ${globals.run_name}

paths:
  checkpoint_dir: epochs/${globals.run_name}
